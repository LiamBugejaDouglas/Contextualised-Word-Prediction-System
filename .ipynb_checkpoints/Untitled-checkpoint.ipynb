{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "335002ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: korpus_malti/shuffled\n",
      "Found cached dataset korpus_malti (C:/Users/lbuge/.cache/huggingface/datasets/MLRS___korpus_malti/shuffled/4.1.0/1e40e18bd238af4ddb9265b251cfdf39d6b7686181dfc2e9bc58f95353949cd5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5245e8dd1d764017839479eb1fc2fe0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 17084545\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3014989\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 401\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import datasets\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import string\n",
    "import csv\n",
    "import itertools\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "tokenizer = ToktokTokenizer()\n",
    "import tensorflow\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from keras.utils import to_categorical\n",
    "import re\n",
    "\n",
    "dataset = datasets.load_dataset(\"MLRS/korpus_malti\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4b53d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define set of punctuation marks\n",
    "punc = set('!\"#$%&\\()*+,./:;<=>?@[\\\\]^_`{|}~§€—–➢•“”☐▌¥')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542b1be5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a list of strings\n",
    "strings = []\n",
    "\n",
    "# Loop through the dataset\n",
    "for x in range(dataset['test'].num_rows-1):\n",
    "    text = str(dataset['test'][x])\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"\\'text\\': \", \"\")\n",
    "    text = text.replace('\\\\n', '')\n",
    "    \n",
    "    # Remove all punctuation marks\n",
    "    text = text.translate(str.maketrans('', '', ''.join(punc)))\n",
    "    #Remove - and add spaces\n",
    "    text = text.translate(str.maketrans(\"-'’\", \"   \"))\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "       \n",
    "    # Remove leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Add string to list if it is long enough\n",
    "    if len(text) > 10:\n",
    "        strings.append(text)\n",
    "\n",
    "# Create dataframe from list of strings\n",
    "df_test = pd.DataFrame({'string_values': strings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891a9bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of strings\n",
    "strings = []\n",
    "\n",
    "# Loop through the dataset\n",
    "for x in range(dataset['train'].num_rows-1):\n",
    "    text = str(dataset['train'][x])\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"\\'text\\': \", \"\")\n",
    "    text = text.replace('\\\\n', '')\n",
    "    \n",
    "    # Remove all punctuation marks using translate() and regular expressions\n",
    "    text = text.translate(str.maketrans('', '', ''.join(punc)))\n",
    "    #Remove - and add spaces\n",
    "    text = text.translate(str.maketrans(\"-'’\", \"   \"))\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Add string to list if it is long enough\n",
    "    if len(text) > 10:\n",
    "        strings.append(text)\n",
    "\n",
    "# Create dataframe from list of strings\n",
    "df_train = pd.DataFrame({'string_values': strings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a04771",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a list of strings\n",
    "strings = []\n",
    "\n",
    "# Loop through the dataset\n",
    "for x in range(dataset['validation'].num_rows-1):\n",
    "    text = str(dataset['validation'][x])\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"\\'text\\': \", \"\")\n",
    "    text = text.replace('\\\\n', '')\n",
    "    \n",
    "    # Remove all punctuation marks using translate() and regular expressions\n",
    "    text = text.translate(str.maketrans('', '', ''.join(punc)))\n",
    "    #Remove - and add spaces\n",
    "    text = text.translate(str.maketrans(\"-'’\", \"   \"))\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Add string to list if it is long enough\n",
    "    if len(text) > 10:\n",
    "        strings.append(text)\n",
    "\n",
    "# Create dataframe from list of strings\n",
    "df_validation = pd.DataFrame({'string_values': strings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4201bcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_test)\n",
    "df_test.isnull().sum()\n",
    "df_test.to_pickle(\"test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801a1ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_validation)\n",
    "df_validation.isnull().sum()\n",
    "df_validation.to_pickle(\"validation.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76baa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train)\n",
    "df_train.isnull().sum()\n",
    "df_train.to_pickle(\"train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcf856fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_pickle(\"test.pkl\")\n",
    "df_validation = pd.read_pickle(\"validation.pkl\")\n",
    "df_train = pd.read_pickle(\"train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "528017ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting top 40,000 words from corpus\n",
    "words = 40000\n",
    "tokenizer = Tokenizer(num_words=words)\n",
    "\n",
    "#Vocabulary Index\n",
    "tokenizer.fit_on_texts(df_train.string_values)\n",
    "\n",
    "#Transform text to sequence of integers\n",
    "train_seq = tokenizer.texts_to_sequences(df_train.string_values)\n",
    "test_seq = tokenizer.texts_to_sequences(df_test.string_values)\n",
    "validation_seq = tokenizer.texts_to_sequences(df_validation.string_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ed0a593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'l': 1, 'ta': 2, 'li': 3, 'tal': 4, 'u': 5, 'il': 6, 'għall': 7, 'fil': 8, 'ma': 9, 'f': 10, 'b': 11, 'għal': 12, 'mill': 13, 'jew': 14, 'fl': 15, 'dan': 16, 'minn': 17, 'fuq': 18, 'biex': 19, 'dwar': 20, 'it': 21, 'ir': 22, 'id': 23, 'lill': 24, 'din': 25, 'is': 26, 'tas': 27, 'kif': 28, 'tat': 29, 'qed': 30, 'kien': 31, 'huwa': 32, 'dawn': 33, 'tad': 34, 'hemm': 35, 'għandu': 36, 'għandhom': 37, 'kummissjoni': 38, 'jekk': 39, 'artikolu': 40, 'tar': 41, 'meta': 42, 'ub': 43, 'huma': 44, 'mod': 45, 'aktar': 46, 'mal': 47, 'se': 48, 'mhux': 49, 'wara': 50, 'regolament': 51, 't': 52, 'd': 53, 'kull': 54, 'fis': 55, 'xi': 56, 'jkun': 57, 'membri': 58, 'oħra': 59, 'tiegħu': 60, 'jista': 61, 'ue': 62, 'hija': 63, 'jiġu': 64, 's': 65, 'għandha': 66, 'tkun': 67, 'ukoll': 68, 'bħala': 69, 'dak': 70, 'r': 71, 'nru': 72, 'fejn': 73, 'tagħhom': 74, 'fi': 75, 'kollha': 76, 'bil': 77, 'ministru': 78, 'sabiex': 79, 'kunsill': 80, 'malta': 81, 'skont': 82, 'unjoni': 83, 'lil': 84, 'jiġi': 85, 'sena': 86, 'x': 87, 'dawk': 88, 'istati': 89, 'ewropea': 90, 'bejn': 91, 'in': 92, 'ikun': 93, 'gvern': 94, 'kemm': 95, 'm': 96, 'fit': 97, 'min': 98, 'wieħed': 99, 'tiġi': 100, 'taħt': 101, 'bl': 102, 'fir': 103, 'jistgħu': 104, 'tagħha': 105, 'informazzjoni': 106, 'ewropew': 107, 'ħafna': 108, 'kienu': 109, 'biss': 110, 'jkunu': 111, 'xogħol': 112, 'billi': 113, 'hekk': 114, 'a': 115, 'kienet': 116, 'hu': 117, 'wkoll': 118, 'ke': 119, 'żmien': 120, 'ewwel': 121, 'imma': 122, 'għax': 123, 'qabel': 124, 'fid': 125, 'istess': 126, 'permezz': 127, 'deċiżjoni': 128, 'kumitat': 129, 'rigward': 130, 'parlament': 131, 'għat': 132, 'sa': 133, 'parti': 134, 'direttiva': 135, 'ikunu': 136, 'data': 137, 'hawn': 138, 'każ': 139, 'għas': 140, 'servizzi': 141, 'użu': 142, 'nazzjonali': 143, 'liġi': 144, 'miżuri': 145, 'prodotti': 146, 'qorti': 147, 'tista': 148, 'suq': 149, 'dik': 150, 'tan': 151, 'illi': 152, 'membru': 153, 'iktar': 154, 'numru': 155, 'ġenerali': 156, 'triq': 157, 'aħħar': 158, 'saħħa': 159, 'iżda': 160, 'livell': 161, 'jien': 162, 'anness': 163, 'lejn': 164, 'tax': 165, 'ieħor': 166, 'għalhekk': 167, 'soċjali': 168, 'ftehim': 169, 'pajjiżi': 170, 'kontra': 171, 'sistema': 172, 'partikolari': 173, 'iż': 174, 'bħal': 175, 'barra': 176, 'awtorità': 177, 'waħda': 178, 'għad': 179, 'minħabba': 180, 'bi': 181, 'politika': 182, 'snin': 183, 'implimentazzjoni': 184, 'skond': 185, 'applikazzjoni': 186, 'issa': 187, 'mingħajr': 188, 'istat': 189, 'żewġ': 190, 'regoli': 191, 'persuna': 192, 'nies': 193, 'dritt': 194, 'ebda': 195, 'n': 196, 'persuni': 197, 'onor': 198, 'għan': 199, 'awtoritajiet': 200}\n",
      "1350532 of unique tokens found\n"
     ]
    }
   ],
   "source": [
    "#Creating dictionary for words with unique index\n",
    "word_index = tokenizer.word_index\n",
    "#Finding top 200 words\n",
    "print(dict(itertools.islice(word_index.items(), 200)))\n",
    "print('%s of unique tokens found'%len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db146f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15761243, 40)\n",
      "(366, 40)\n",
      "(2781684, 40)\n"
     ]
    }
   ],
   "source": [
    "#Adding padding so each sequence has equal length\n",
    "training_data = pad_sequences(train_seq, maxlen=40).astype(np.uint8)\n",
    "testing_data = pad_sequences(test_seq, maxlen=40).astype(np.uint8)\n",
    "validate_data = pad_sequences(validation_seq, maxlen=40).astype(np.uint8)\n",
    "\n",
    "print(training_data.shape)\n",
    "print(testing_data.shape)\n",
    "print(validate_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5202333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6df2fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, labels_train = training_data[:,:-1],training_data[:,-1]\n",
    "train_y = to_categorical(labels_train)\n",
    "\n",
    "val_x, labels_val = validate_data[:,:-1],validate_data[:,-1]\n",
    "val_y = to_categorical(labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ef29ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...  17 253  50]\n",
      " [  0   0   0 ... 164  18 240]\n",
      " [  0   0   0 ...   4  94   2]\n",
      " ...\n",
      " [  0   0   0 ...  43 225   5]\n",
      " [  0   0   0 ...  44   5 201]\n",
      " [  0   0   0 ...   5  38 184]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(train_x)\n",
    "print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4ab383f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\lbuge\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\lbuge\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\lbuge\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\lbuge\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\lbuge\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\lbuge\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\lbuge\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\lbuge\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\lbuge\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 2176, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"C:\\Users\\lbuge\\anaconda3\\lib\\site-packages\\keras\\backend.py\", line 5680, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 40000) vs (None, 256)).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15472\\2957542314.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m#print model.summary()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\lbuge\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\lbuge\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\lbuge\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\lbuge\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\lbuge\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\lbuge\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\lbuge\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\lbuge\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\lbuge\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 2176, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"C:\\Users\\lbuge\\anaconda3\\lib\\site-packages\\keras\\backend.py\", line 5680, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 40000) vs (None, 256)).\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(words, 100, input_length=39))\n",
    "model.add(Bidirectional(LSTM(150)))\n",
    "model.add(Dense(words, activation='softmax'))\n",
    "model.compile(loss = 'sparse_categorical_crossentropy',optimizer='adam',metrics = ['accuracy'])\n",
    "history = model.fit(train_x, train_y, epochs=50, verbose=1)\n",
    "#print model.summary()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ad0741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

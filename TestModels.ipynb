{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pyaudio\n",
    "import wave\n",
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "from tensorflow.keras.models import load_model\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Multimodal Dataset\\Text\\Bathroom.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "lines = text.split('\\n')\n",
    "\n",
    "punctuation = string.punctuation.replace(\"'\", \"\")\n",
    "\n",
    "x=0\n",
    "\n",
    "data = []\n",
    "for i in range(len(lines)-1):\n",
    "    if lines[i].startswith('Person 2'):\n",
    "        p2 = lines[i][9:].strip().replace('\"', '')\n",
    "        # Process contractions\n",
    "        p2 = re.sub(r\"([a-zA-Z])'([a-zA-Z])\", r\"\\1 '\\2\", p2)\n",
    "        p2 = ''.join([char for char in p2 if char not in punctuation])\n",
    "        \n",
    "        data.append({\n",
    "            'Location': 'Multimodal Dataset\\Images\\Bathroom\\\\bath_' + str(x + 1)+'.jpg',\n",
    "            'Person 1': 'Multimodal Dataset\\Speech\\Bathroom\\Bathroom-' + str(x + 1)+'.wav',\n",
    "            'Person 2': p2\n",
    "        })\n",
    "        x += 1\n",
    "\n",
    "df1 = pd.DataFrame(data)\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Multimodal Dataset\\Text\\Bedroom.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "lines = text.split('\\n')\n",
    "\n",
    "punctuation = string.punctuation.replace(\"'\", \"\")\n",
    "\n",
    "x=0\n",
    "\n",
    "data = []\n",
    "for i in range(len(lines)-1):\n",
    "    if lines[i].startswith('Person 2'):\n",
    "        p2 = lines[i][9:].strip().replace('\"', '')\n",
    "        # Process contractions\n",
    "        p2 = re.sub(r\"([a-zA-Z])'([a-zA-Z])\", r\"\\1 '\\2\", p2)\n",
    "        p2 = ''.join([char for char in p2 if char not in punctuation])\n",
    "        \n",
    "        data.append({\n",
    "            'Location': '\\Multimodal Dataset\\Images\\Bedroom\\\\bed_' + str(x + 1)+'.jpg',\n",
    "            'Person 1': '\\Multimodal Dataset\\Speech\\Bedroom\\Bedroom-' + str(x + 1)+'.wav',\n",
    "            'Person 2': p2\n",
    "        })\n",
    "        x += 1\n",
    "\n",
    "df2 = pd.DataFrame(data)\n",
    "print(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Multimodal Dataset\\Text\\DiningRoom.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "lines = text.split('\\n')\n",
    "\n",
    "punctuation = string.punctuation.replace(\"'\", \"\")\n",
    "\n",
    "x=0\n",
    "\n",
    "data = []\n",
    "for i in range(len(lines)-1):\n",
    "    if lines[i].startswith('Person 2'):\n",
    "        p2 = lines[i][9:].strip().replace('\"', '')\n",
    "        # Process contractions\n",
    "        p2 = re.sub(r\"([a-zA-Z])'([a-zA-Z])\", r\"\\1 '\\2\", p2)\n",
    "        p2 = ''.join([char for char in p2 if char not in punctuation])\n",
    "        \n",
    "        data.append({\n",
    "            'Location': '\\Multimodal Dataset\\Images\\DiningRoom\\din_' + str(x + 1)+'.jpg',\n",
    "            'Person 1': '\\Multimodal Dataset\\Speech\\DiningRoom\\Dining-' + str(x + 1)+'.wav',\n",
    "            'Person 2': p2\n",
    "        })\n",
    "        x += 1\n",
    "\n",
    "df3 = pd.DataFrame(data)\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Multimodal Dataset\\Text\\Kitchen.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "lines = text.split('\\n')\n",
    "\n",
    "punctuation = string.punctuation.replace(\"'\", \"\")\n",
    "\n",
    "x=0\n",
    "\n",
    "data = []\n",
    "for i in range(len(lines)-1):\n",
    "    if lines[i].startswith('Person 2'):\n",
    "        p2 = lines[i][9:].strip().replace('\"', '')\n",
    "        # Process contractions\n",
    "        p2 = re.sub(r\"([a-zA-Z])'([a-zA-Z])\", r\"\\1 '\\2\", p2)\n",
    "        p2 = ''.join([char for char in p2 if char not in punctuation])\n",
    "        \n",
    "        data.append({\n",
    "            'Location': '\\Multimodal Dataset\\Images\\Kitchen\\kitchen_' + str(x + 1)+'.jpg',\n",
    "            'Person 1': '\\Multimodal Dataset\\Speech\\Kitchen\\Kitchen-' + str(x + 1)+'.wav',\n",
    "            'Person 2': p2\n",
    "        })\n",
    "        x += 1\n",
    "\n",
    "df4 = pd.DataFrame(data)\n",
    "print(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Multimodal Dataset\\Text\\LivingRoom.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "lines = text.split('\\n')\n",
    "\n",
    "punctuation = string.punctuation.replace(\"'\", \"\")\n",
    "\n",
    "x=0\n",
    "\n",
    "data = []\n",
    "for i in range(len(lines)-1):\n",
    "    if lines[i].startswith('Person 2'):\n",
    "        p2 = lines[i][9:].strip().replace('\"', '')\n",
    "        # Process contractions\n",
    "        p2 = re.sub(r\"([a-zA-Z])'([a-zA-Z])\", r\"\\1 '\\2\", p2)\n",
    "        p2 = ''.join([char for char in p2 if char not in punctuation])\n",
    "        \n",
    "        data.append({\n",
    "            'Location': '\\Multimodal Dataset\\Images\\LivingRoom\\living_' + str(x + 1)+'.jpg',\n",
    "            'Person 1': '\\Multimodal Dataset\\Speech\\LivingRoom\\Living-' + str(x + 1)+'.wav',\n",
    "            'Person 2': p2\n",
    "        })\n",
    "        x += 1\n",
    "\n",
    "df5 = pd.DataFrame(data)\n",
    "print(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2, df3, df4, df5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_room_classification(loc):\n",
    "\n",
    "    model = load_model('models/VGG19-Classification.h5')  \n",
    "    img = cv.imread(loc)\n",
    "    resize = tf.image.resize(img, (224, 224))\n",
    "    input_image = np.expand_dims(resize / 255, 0)\n",
    "    yhat = model.predict(input_image)\n",
    "    predicted_class_index = np.argmax(yhat)\n",
    "    labels = ['Bathroom', 'Bedroom', 'Dinning', 'Kitchen', 'Living Room']\n",
    "    predicted_label = labels[predicted_class_index]\n",
    "    final= \"User is currently in the \"+ predicted_label + \".\"\n",
    "\n",
    "    return final\n",
    "\n",
    "def get_audio_text(loc):\n",
    "    r = sr.Recognizer()\n",
    "\n",
    "    with sr.AudioFile(loc) as source:\n",
    "        audio_text = r.listen(source)\n",
    "    \n",
    "\n",
    "    return r.recognize_google(audio_text, show_all=False) + \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFRobertaForMaskedLM\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = TFRobertaForMaskedLM.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roberta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to keep track of correct and total predictions\n",
    "num_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Loop through each row in the dataframe\n",
    "for i, row in df.iterrows():\n",
    "\n",
    "    # Split Person 2's dialogue into individual words\n",
    "    words = row['Person 2'].split()\n",
    "\n",
    "    # Loop through each word in Person 2's dialogue\n",
    "    for j in range(len(words)):\n",
    "\n",
    "        before_mask = ' '.join(words[:j])\n",
    "        after_mask = ' '.join(words[j+1:])\n",
    "\n",
    "        {row['Location']}, {row['Person 1']},\n",
    "\n",
    "        \n",
    "        room = get_room_classification(row['Location'])\n",
    "        audio = get_audio_text(row['Person 1'])\n",
    "        \n",
    "        \n",
    "        # Construct the input string\n",
    "        input_string = f\"{row['Person 1']} {before_mask} <mask>\"\n",
    "\n",
    "        # Tokenize the input string\n",
    "        inputs = tokenizer(input_string, return_tensors=\"tf\")\n",
    "        logits = model(**inputs).logits\n",
    "        mask_token_index = tf.where((inputs.input_ids == tokenizer.mask_token_id)[0])\n",
    "        selected_logits = tf.gather_nd(logits[0], indices=mask_token_index)\n",
    "        predicted_token_id = tf.math.argmax(selected_logits, axis=-1)\n",
    "        predicted_word = tokenizer.decode(predicted_token_id)\n",
    "\n",
    "        \n",
    "        while predicted_word in string.punctuation or predicted_word == \"</s>\":\n",
    "            # Set the predicted token id to the next highest value\n",
    "            selected_logits = tf.where(selected_logits == tf.reduce_max(selected_logits),tf.constant(-1e9, dtype=tf.float32, shape=selected_logits.shape),selected_logits)\n",
    "            predicted_token_id = tf.math.argmax(selected_logits, axis=-1)\n",
    "            predicted_word = tokenizer.decode(predicted_token_id)\n",
    "            predicted_word = predicted_word.strip()\n",
    "\n",
    "        # Check if the predicted token is correct\n",
    "        if predicted_word.replace(\" \",\"\") == words[j].replace(\" \", \"\"):\n",
    "            num_correct += 1\n",
    "\n",
    "        total_predictions += 1\n",
    "\n",
    "# Print the number of correct predictions and the total number of predictions\n",
    "print(f\"Number of correct predictions: {num_correct}\")\n",
    "print(f\"Total number of predictions: {total_predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to keep track of correct and total predictions\n",
    "num_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Loop through each row in the dataframe\n",
    "for i, row in df.iterrows():\n",
    "\n",
    "    # Split Person 2's dialogue into individual words\n",
    "    words = row['Person 2'].split()\n",
    "\n",
    "    # Loop through each word in Person 2's dialogue\n",
    "    for j in range(len(words)):\n",
    "\n",
    "        before_mask = ' '.join(words[:j])\n",
    "        after_mask = ' '.join(words[j+1:])\n",
    "\n",
    "        # Construct the input string\n",
    "        input_string = f\"{before_mask} <mask>\"\n",
    "\n",
    "        # Tokenize the input string\n",
    "        inputs = tokenizer(input_string, return_tensors=\"tf\")\n",
    "        logits = model(**inputs).logits\n",
    "        mask_token_index = tf.where((inputs.input_ids == tokenizer.mask_token_id)[0])\n",
    "        selected_logits = tf.gather_nd(logits[0], indices=mask_token_index)\n",
    "        predicted_token_id = tf.math.argmax(selected_logits, axis=-1)\n",
    "        predicted_word = tokenizer.decode(predicted_token_id)\n",
    "\n",
    "        \n",
    "        while predicted_word in string.punctuation or predicted_word == \"</s>\":\n",
    "            # Set the predicted token id to the next highest value\n",
    "            selected_logits = tf.where(selected_logits == tf.reduce_max(selected_logits),tf.constant(-1e9, dtype=tf.float32, shape=selected_logits.shape),selected_logits)\n",
    "            predicted_token_id = tf.math.argmax(selected_logits, axis=-1)\n",
    "            predicted_word = tokenizer.decode(predicted_token_id)\n",
    "            predicted_word = predicted_word.strip()\n",
    "\n",
    "        # Check if the predicted token is correct\n",
    "        if predicted_word.replace(\" \",\"\") == words[j].replace(\" \", \"\"):\n",
    "            num_correct += 1\n",
    "\n",
    "        total_predictions += 1\n",
    "\n",
    "# Print the number of correct predictions and the total number of predictions\n",
    "print(f\"Number of correct predictions: {num_correct}\")\n",
    "print(f\"Total number of predictions: {total_predictions}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELECTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFElectraForMaskedLM\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/electra-small-generator\")\n",
    "model = TFElectraForMaskedLM.from_pretrained(\"google/electra-small-generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to keep track of correct and total predictions\n",
    "num_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Loop through each row in the dataframe\n",
    "for i, row in df.iterrows():\n",
    "\n",
    "    # Split Person 2's dialogue into individual words\n",
    "    words = row['Person 2'].split()\n",
    "\n",
    "    # Loop through each word in Person 2's dialogue\n",
    "    for j in range(len(words)):\n",
    "\n",
    "        before_mask = ' '.join(words[:j])\n",
    "        after_mask = ' '.join(words[j+1:])\n",
    "\n",
    "        {row['Location']}, {row['Person 1']}, \n",
    "\n",
    "        room = get_room_classification(row['Location'])\n",
    "        audio = get_audio_text(row['Person 1'])\n",
    "\n",
    "        # Construct the input string\n",
    "        input_string = f\"{row['Person 1']} {before_mask} [MASK]\"\n",
    "\n",
    "        # Tokenize the input string\n",
    "        inputs = tokenizer(input_string, return_tensors=\"tf\")\n",
    "        logits = model(**inputs).logits\n",
    "        mask_token_index = tf.where((inputs.input_ids == tokenizer.mask_token_id)[0])\n",
    "        selected_logits = tf.gather_nd(logits[0], indices=mask_token_index)\n",
    "        predicted_token_id = tf.math.argmax(selected_logits, axis=-1)\n",
    "        predicted_word = tokenizer.decode(predicted_token_id)\n",
    "        \n",
    "        while predicted_word in string.punctuation or predicted_word == \"[PAD]\" or predicted_word == \"[UNK]\":\n",
    "            # Set the predicted token id to the next highest value\n",
    "            selected_logits = tf.where(selected_logits == tf.reduce_max(selected_logits),tf.constant(-1e9, dtype=tf.float32, shape=selected_logits.shape),selected_logits)\n",
    "            predicted_token_id = tf.math.argmax(selected_logits, axis=-1)\n",
    "            predicted_word = tokenizer.decode(predicted_token_id)\n",
    "            predicted_word = predicted_word.strip()\n",
    "\n",
    "        # Check if the predicted token is correct\n",
    "        if predicted_word.replace(\" \",\"\") == words[j].replace(\" \", \"\"):\n",
    "            num_correct += 1\n",
    "\n",
    "\n",
    "        total_predictions += 1\n",
    "\n",
    "# Print the number of correct predictions and the total number of predictions\n",
    "print(f\"Number of correct predictions: {num_correct}\")\n",
    "print(f\"Total number of predictions: {total_predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to keep track of correct and total predictions\n",
    "num_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Loop through each row in the dataframe\n",
    "for i, row in df.iterrows():\n",
    "\n",
    "    # Split Person 2's dialogue into individual words\n",
    "    words = row['Person 2'].split()\n",
    "\n",
    "    # Loop through each word in Person 2's dialogue\n",
    "    for j in range(len(words)):\n",
    "\n",
    "        before_mask = ' '.join(words[:j])\n",
    "        after_mask = ' '.join(words[j+1:])\n",
    "\n",
    "        # Construct the input string\n",
    "        input_string = f\"{before_mask} [MASK]\"\n",
    "\n",
    "        # Tokenize the input string\n",
    "        inputs = tokenizer(input_string, return_tensors=\"tf\")\n",
    "        logits = model(**inputs).logits\n",
    "        mask_token_index = tf.where((inputs.input_ids == tokenizer.mask_token_id)[0])\n",
    "        selected_logits = tf.gather_nd(logits[0], indices=mask_token_index)\n",
    "        predicted_token_id = tf.math.argmax(selected_logits, axis=-1)\n",
    "        predicted_word = tokenizer.decode(predicted_token_id)\n",
    "\n",
    "        while predicted_word in string.punctuation or predicted_word == \"[PAD]\" or predicted_word == \"[UNK]\":\n",
    "            # Set the predicted token id to the next highest value\n",
    "            selected_logits = tf.where(selected_logits == tf.reduce_max(selected_logits),tf.constant(-1e9, dtype=tf.float32, shape=selected_logits.shape),selected_logits)\n",
    "            predicted_token_id = tf.math.argmax(selected_logits, axis=-1)\n",
    "            predicted_word = tokenizer.decode(predicted_token_id)\n",
    "            predicted_word = predicted_word.strip()\n",
    "    \n",
    "        # Check if the predicted token is correct\n",
    "        if predicted_word.replace(\" \",\"\") == words[j].replace(\" \", \"\"):\n",
    "            num_correct += 1\n",
    "\n",
    "        total_predictions += 1\n",
    "\n",
    "# Print the number of correct predictions and the total number of predictions\n",
    "print(f\"Number of correct predictions: {num_correct}\")\n",
    "print(f\"Total number of predictions: {total_predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFBertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "model = TFBertForMaskedLM.from_pretrained(\"bert-large-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to keep track of correct and total predictions\n",
    "num_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Loop through each row in the dataframe\n",
    "for i, row in df.iterrows():\n",
    "\n",
    "    # Split Person 2's dialogue into individual words\n",
    "    words = row['Person 2'].split()\n",
    "\n",
    "    # Loop through each word in Person 2's dialogue\n",
    "    for j in range(len(words)):\n",
    "\n",
    "        before_mask = ' '.join(words[:j])\n",
    "        after_mask = ' '.join(words[j+1:])\n",
    "\n",
    "        {row['Location']}, {row['Person 1']}, \n",
    "\n",
    "        room = get_room_classification(row['Location'])\n",
    "        audio = get_audio_text(row['Person 1'])\n",
    "\n",
    "        # Construct the input string\n",
    "        input_string = f\"{row['Person 1']} {before_mask} [MASK]\"\n",
    "\n",
    "        # Tokenize the input string\n",
    "        inputs = tokenizer(input_string, return_tensors=\"tf\")\n",
    "        logits = model(**inputs).logits\n",
    "        mask_token_index = tf.where((inputs.input_ids == tokenizer.mask_token_id)[0])\n",
    "        selected_logits = tf.gather_nd(logits[0], indices=mask_token_index)\n",
    "        predicted_token_id = tf.math.argmax(selected_logits, axis=-1)\n",
    "        predicted_word = tokenizer.decode(predicted_token_id)\n",
    "\n",
    "        \n",
    "        while predicted_word in string.punctuation or predicted_word == \"</s>\":\n",
    "            # Set the predicted token id to the next highest value\n",
    "            selected_logits = tf.where(selected_logits == tf.reduce_max(selected_logits),tf.constant(-1e9, dtype=tf.float32, shape=selected_logits.shape),selected_logits)\n",
    "            predicted_token_id = tf.math.argmax(selected_logits, axis=-1)\n",
    "            predicted_word = tokenizer.decode(predicted_token_id)\n",
    "            predicted_word = predicted_word.strip()\n",
    "\n",
    "        # Check if the predicted token is correct\n",
    "        if predicted_word.replace(\" \",\"\") == words[j].replace(\" \", \"\"):\n",
    "            num_correct += 1\n",
    "\n",
    "        total_predictions += 1\n",
    "\n",
    "# Print the number of correct predictions and the total number of predictions\n",
    "print(f\"Number of correct predictions: {num_correct}\")\n",
    "print(f\"Total number of predictions: {total_predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to keep track of correct and total predictions\n",
    "num_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Loop through each row in the dataframe\n",
    "for i, row in df.iterrows():\n",
    "\n",
    "    # Split Person 2's dialogue into individual words\n",
    "    words = row['Person 2'].split()\n",
    "\n",
    "    # Loop through each word in Person 2's dialogue\n",
    "    for j in range(len(words)):\n",
    "\n",
    "        before_mask = ' '.join(words[:j])\n",
    "        after_mask = ' '.join(words[j+1:])\n",
    "\n",
    "        {row['Location']}, {row['Person 1']}, \n",
    "\n",
    "        # Construct the input string\n",
    "        input_string = f\"{before_mask} [MASK]\"\n",
    "\n",
    "        # Tokenize the input string\n",
    "        inputs = tokenizer(input_string, return_tensors=\"tf\")\n",
    "        logits = model(**inputs).logits\n",
    "        mask_token_index = tf.where((inputs.input_ids == tokenizer.mask_token_id)[0])\n",
    "        selected_logits = tf.gather_nd(logits[0], indices=mask_token_index)\n",
    "        predicted_token_id = tf.math.argmax(selected_logits, axis=-1)\n",
    "        predicted_word = tokenizer.decode(predicted_token_id)\n",
    "\n",
    "        \n",
    "        while predicted_word in string.punctuation or predicted_word == \"</s>\":\n",
    "            # Set the predicted token id to the next highest value\n",
    "            selected_logits = tf.where(selected_logits == tf.reduce_max(selected_logits),tf.constant(-1e9, dtype=tf.float32, shape=selected_logits.shape),selected_logits)\n",
    "            predicted_token_id = tf.math.argmax(selected_logits, axis=-1)\n",
    "            predicted_word = tokenizer.decode(predicted_token_id)\n",
    "            predicted_word = predicted_word.strip()\n",
    "\n",
    "        # Check if the predicted token is correct\n",
    "        if predicted_word.replace(\" \",\"\") == words[j].replace(\" \", \"\"):\n",
    "            num_correct += 1\n",
    "\n",
    "        total_predictions += 1\n",
    "\n",
    "# Print the number of correct predictions and the total number of predictions\n",
    "print(f\"Number of correct predictions: {num_correct}\")\n",
    "print(f\"Total number of predictions: {total_predictions}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FinalFYP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
